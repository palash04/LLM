{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i1qbcKFkL_s4"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  src_vocab_size: int = 32005\n",
        "  tgt_vocab_size: int = 32005\n",
        "  src_seq_len: int = 1024\n",
        "  tgt_seq_len: int = 1024\n",
        "  d_model: int = 512\n",
        "  n_heads: int = 8\n",
        "  n_layers: int = 6\n",
        "  dropout: float = 0.1\n",
        "  d_ff: int = 2048"
      ],
      "metadata": {
        "id": "rn_bLcc165WH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InputEmbeddings"
      ],
      "metadata": {
        "id": "sgRUkIcE7x-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T) -> (B, T, d_model)\n",
        "    return self.embedding(x)"
      ],
      "metadata": {
        "id": "9hB4Yh9QMkEC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PositionalEncoding"
      ],
      "metadata": {
        "id": "0hyA4dMl70Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    pe = torch.zeros(seq_len, d_model)    # (T, d_model)\n",
        "\n",
        "    # create a vector of shape seq_len\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)   # (T, 1)\n",
        "\n",
        "    # create a vector of shape d_model\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).unsqueeze(0)   # (1, d_model/2)\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)    # (1, T, d_model)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, d_model)\n",
        "    # x: (B, T, d_model)\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)  # (B, T, d_model)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "_4O0oLKjUc8S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LayerNormalization"
      ],
      "metadata": {
        "id": "9g-YQoLuBXfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, d_model: int, eps: float=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(d_model))   # learnable parameter\n",
        "    self.bias = nn.Parameter(torch.zeros(d_model))   # learnable parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, T, d_model)\n",
        "    mean = x.mean(dim=-1, keepdim=True)    # (B, T, 1)\n",
        "    std = x.std(dim=-1, keepdim=True)      # (B, T, 1)\n",
        "    x = self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "    return x"
      ],
      "metadata": {
        "id": "GPHPSIOiBWAF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResidualConnection"
      ],
      "metadata": {
        "id": "5litTwI3DPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    dropout = config.dropout\n",
        "\n",
        "    self.norm = LayerNormalization(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    # (B, T, d_model) -> (B, T, d_model)\n",
        "    x = x + self.dropout(sublayer(self.norm(x)))\n",
        "    return x"
      ],
      "metadata": {
        "id": "DxvDbUL-DPLe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForwardBlock"
      ],
      "metadata": {
        "id": "mKn_IqUzIpXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    d_ff = config.d_ff\n",
        "    dropout = config.dropout\n",
        "\n",
        "    self.up_proj = nn.Linear(d_model, d_ff)\n",
        "    self.down_proj = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, d_ff) -> (B, T, d_model)\n",
        "    x = self.dropout(self.relu(self.up_proj(x)))\n",
        "    x = self.down_proj(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jERiweHUBV9Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttentionBlock"
      ],
      "metadata": {
        "id": "k9aEdkdnKR5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = config.d_model\n",
        "    self.n_heads = config.n_heads\n",
        "    self.dropout = config.dropout\n",
        "\n",
        "    assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "    self.head_size = self.d_model // self.n_heads\n",
        "    self.w_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.w_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.w_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "    self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    # return tensor of shape: (B, T, d_model)\n",
        "    # q, k, v : (B, T, d_model)\n",
        "    B, T = q.shape[0], q.shape[1]\n",
        "    query = self.w_q(q)   # (B, T, d_model)\n",
        "    key = self.w_k(k)     # (B, T, d_model)\n",
        "    value = self.w_v(q)   # (B, T, d_model)\n",
        "\n",
        "    query = query.view(B, T, self.n_heads, self.head_size)  # (B, T, nH, head_size)\n",
        "    key = key.view(B, T, self.n_heads, self.head_size)  # (B, T, nH, head_size)\n",
        "    value = value.view(B, T, self.n_heads, self.head_size)  # (B, T, nH, head_size)\n",
        "\n",
        "    query = query.transpose(1, 2)    # (B, nH, T, head_size)\n",
        "    key = key.transpose(1, 2)    # (B, nH, T, head_size)\n",
        "    value = value.transpose(1, 2)    # (B, nH, T, head_size)\n",
        "\n",
        "    attention_scores = query @ key.transpose(2, 3) / math.sqrt(self.head_size)       # (B, nh, T, T)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores = attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    if self.dropout is not None:\n",
        "      attention_scores = self.dropout(attention_scores)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)    # (B, nH, T, T)\n",
        "\n",
        "    z = attention_scores @ value    # (B, nH, T, head_size)\n",
        "\n",
        "    z = z.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "\n",
        "    return self.w_o(z)"
      ],
      "metadata": {
        "id": "0ksiy9WAKSQ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EncoderBlock"
      ],
      "metadata": {
        "id": "4wka-6FPPies"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, config: Config, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(config) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # x: (B, T, d_model)\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, mask))\n",
        "    x = self.residual_connections[0](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "id": "U6mY_2r2PiI7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DecoderBlock"
      ],
      "metadata": {
        "id": "NiGtBkc1PkjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, config: Config, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(config) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "    x = self.residual_connections[0](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "1Voe2lZpKSqg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "LnyX3fUtVuaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config: Config, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(d_model)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "OXn-UwlkO9uu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "YsOyUeOyVwXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config: Config, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    self.norm = LayerNormalization(d_model)\n",
        "    self.layers = layers\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "uujm1pZZVwLD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ProjectionLayer"
      ],
      "metadata": {
        "id": "7b37Iigsv1QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, T, d_model) -> (B, T, vocab_size)\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "id": "OvXoJBkorzwX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "tJXTWHJIv28O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, x, mask):\n",
        "    # x: (B, T)\n",
        "    x = self.src_embed(x)\n",
        "    x = self.src_pos(x)\n",
        "    x = self.encoder(x, mask)   # (B, T, d_model)\n",
        "    return x\n",
        "\n",
        "  def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "    # tgt: (B, T)\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "0NsxvO3NrcdO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function to build transformer model"
      ],
      "metadata": {
        "id": "kwPXGcR-v41y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(config: Config):\n",
        "  src_embed = InputEmbeddings(config.d_model, config.src_vocab_size)\n",
        "  tgt_embed = InputEmbeddings(config.d_model, config.tgt_vocab_size)\n",
        "\n",
        "  src_pos = PositionalEncoding(config.d_model, config.src_seq_len, config.dropout)\n",
        "  tgt_pos = PositionalEncoding(config.d_model, config.tgt_seq_len, config.dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "  for _ in range(config.n_layers):\n",
        "    self_attention_block = MultiHeadAttentionBlock(config)\n",
        "    feed_forward_block = FeedForwardBlock(config)\n",
        "    encoder_block = EncoderBlock(config, self_attention_block, feed_forward_block)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks = []\n",
        "  for _ in range(config.n_layers):\n",
        "    self_attention_block = MultiHeadAttentionBlock(config)\n",
        "    cross_attention_block = MultiHeadAttentionBlock(config)\n",
        "    feed_forward_block = FeedForwardBlock(config)\n",
        "    decoder_block = DecoderBlock(config, self_attention_block, cross_attention_block, feed_forward_block)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder = Encoder(config, nn.ModuleList(encoder_blocks))\n",
        "  decoder = Decoder(config, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  projection_layer = ProjectionLayer(config.d_model, config.tgt_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "bG61h9utq_nD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMVKcQm7vTHz",
        "outputId": "5931ba03-806f-4284-b729-5729babc32a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(src_vocab_size=32005, tgt_vocab_size=32005, src_seq_len=1024, tgt_seq_len=1024, d_model=512, n_heads=8, n_layers=6, dropout=0.1, d_ff=2048)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_transformer(config)"
      ],
      "metadata": {
        "id": "Sanrtg-Eq_it"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOPDPeZYVDEU",
        "outputId": "173645c8-932b-46d1-8778-325c16e6f19e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (norm): LayerNormalization()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (norm): LayerNormalization()\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (norm): LayerNormalization()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(32005, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(32005, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): ProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=32005, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ncbz4PQ_v_Gl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "JcKwIGdywony"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "BfBk-tALT3zN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "GC4KOh9sT3no"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('Helsinki-NLP/opus_books', 'en-fr', split='train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB_gDQOJUJM5",
        "outputId": "44926e34-d18f-43ed-b843-99e48904c12a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "idx = random.randint(0, len(dataset) - 1)\n",
        "\n",
        "print('English: ')\n",
        "print(dataset[idx]['translation']['en'])\n",
        "print('French: ')\n",
        "print(dataset[idx]['translation']['fr'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo-NEmipUI9J",
        "outputId": "546005f3-a818-48ff-d25c-9dfb603f123f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: \n",
            "\"Let her be then, old man! It's the Piolaine young lady,\" cried Maheude to the grandfather, recognizing Cécile, whose veil had been torn off by one of the women.\n",
            "French: \n",
            "—Laissez-la donc, vieux! c'est la demoiselle de la Piolaine! cria la Maheude au grand-pere, en reconnaissant Cécile, dont une femme avait déchiré la voilette.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EF_Bzw1Ux0D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "Let's use llama tokenizer"
      ],
      "metadata": {
        "id": "NhQZKh30VQ5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "login(hf_token)\n",
        "print(\"Logged in successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZ5U4eWW4ZN",
        "outputId": "7145efb3-a2c0-4523-e2ae-678f27db9d17"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Logged in successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "jQF4KMTWUxin"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_id = 'meta-llama/Llama-2-7b-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6J5Fc6QViVD",
        "outputId": "ed495a2c-bc9e-4943-a2f1-f93dc1e93125"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T7F-CWgg3yh",
        "outputId": "36be4644-1bc5-4bfc-fb50-7f846c52f655"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeK-x9fHg5we",
        "outputId": "f11a98e1-89a3-4411-d7e5-0e6b033a432e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('Hello, how are you?')[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6le-JUuYrAA",
        "outputId": "e1d3e79f-89a3-489a-f5fd-ffdd35b0c426"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15043, 29892, 920, 526, 366, 29973]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(1), tokenizer.decode(2), tokenizer.decode(32000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dfbvkZda5Vu",
        "outputId": "cd132d24-80fe-4583-9e15-d51cd248cd3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<s>', '</s>', '<pad>')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoader"
      ],
      "metadata": {
        "id": "IBUc6I-hYrch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "  def __init__(self, ds, tokenizer, seq_len):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "    self.ds = ds\n",
        "    self.tokenizer = tokenizer\n",
        "    self.bos_token_id = tokenizer.bos_token_id\n",
        "    self.eos_token_id = tokenizer.eos_token_id\n",
        "    self.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src_tgt_pair = self.ds[idx]\n",
        "    src_text = src_tgt_pair['translation']['en']\n",
        "    tgt_text = src_tgt_pair['translation']['fr']\n",
        "\n",
        "    # tokenize texts\n",
        "    src_tokens = self.tokenizer.encode(src_text)[1:]   # [1, .....]\n",
        "    tgt_tokens = self.tokenizer.encode(tgt_text)[1:]   # [1, .....]\n",
        "\n",
        "    src_num_padding_tokens = self.seq_len - len(src_tokens) - 2     # -2 because of sos_token, eos_token\n",
        "    tgt_num_padding_tokens = self.seq_len - len(tgt_tokens) - 1     # we only add sos token to decoder side\n",
        "\n",
        "    if src_num_padding_tokens < 0 or tgt_num_padding_tokens < 0:\n",
        "      raise ValueError(f\"Sentence with id: {idx} is too long\")\n",
        "\n",
        "    # Add </s> token, <s> token is already added\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            torch.tensor([self.bos_token_id]),\n",
        "            torch.tensor(src_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.eos_token_id]),\n",
        "            torch.tensor([self.pad_token_id] * src_num_padding_tokens, dtype=torch.int64),\n",
        "\n",
        "        ],\n",
        "        dim=0\n",
        "    )\n",
        "\n",
        "    # add only <s> to decoder side\n",
        "    decoder_input = torch.cat(\n",
        "        [\n",
        "            torch.tensor([self.bos_token_id]),\n",
        "            torch.tensor(tgt_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token_id] * tgt_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0\n",
        "    )\n",
        "\n",
        "    # add only </s> to the label\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(tgt_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.eos_token_id]),\n",
        "            torch.tensor([self.pad_token_id] * tgt_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0) == self.seq_len\n",
        "    assert decoder_input.size(0) == self.seq_len\n",
        "    assert label.size(0) == self.seq_len\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,   # (seq_len)\n",
        "        \"decoder_input\": decoder_input,   # (seq_len)\n",
        "        \"encoder_mask\": (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),    # (1, 1, seq_len)\n",
        "        \"decoder_mask\": (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),    # (1, 1, seq_len) & (1, seq_len, seq_len)\n",
        "        \"label\": label,  # (seq_len)\n",
        "        \"src_text\": src_text,\n",
        "        \"tgt_text\": tgt_text,\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "  return mask == 0\n"
      ],
      "metadata": {
        "id": "-qVWhmZLZJAP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "train_ds_size = int(0.9 * len(dataset))\n",
        "val_ds_size = len(dataset) - train_ds_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_ds_size, val_ds_size])\n",
        "\n",
        "train_ds = BilingualDataset(train_ds, tokenizer, seq_len=1024)\n",
        "val_ds = BilingualDataset(val_ds, tokenizer, seq_len=1024)\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_ds, batch_size=1)"
      ],
      "metadata": {
        "id": "ETL_fRS7vexl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "Zypvp9E9xDde"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu0Gya65xC_j",
        "outputId": "a1cf7174-9510-4bf3-c139-7663fc31d13c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('encoder_input: ', data['encoder_input'].shape)\n",
        "print('decoder_input: ', data['decoder_input'].shape)\n",
        "print('encoder_mask: ', data['encoder_mask'].shape)\n",
        "print('decoder_mask: ', data['decoder_mask'].shape)\n",
        "print('label: ', data['label'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw9sYgVlxdTy",
        "outputId": "6fec77b6-3921-4f93-ff82-44a889ea250e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input:  torch.Size([4, 1024])\n",
            "decoder_input:  torch.Size([4, 1024])\n",
            "encoder_mask:  torch.Size([4, 1, 1, 1024])\n",
            "decoder_mask:  torch.Size([4, 1, 1024, 1024])\n",
            "label:  torch.Size([4, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXJBRCuhxdJt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "DMmDjNU6wrle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5RZENjZo8IIQ",
        "outputId": "3a00110b-9722-4517-a427-87df71ef4983"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QSTgL3a8Gsr",
        "outputId": "a9b17334-68ee-4875-a7eb-49ad72283a7d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (norm): LayerNormalization()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (norm): LayerNormalization()\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (up_proj): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (down_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (norm): LayerNormalization()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(32005, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(32005, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): ProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=32005, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "UtM0X3u8v_BF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)"
      ],
      "metadata": {
        "id": "Kt9YfKP80IOU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sneak Peak into crossentropyloss"
      ],
      "metadata": {
        "id": "jJNAvXO62I-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 2\n",
        "Classes = 10\n",
        "pred = torch.rand(B, Classes)\n",
        "pred = pred.softmax(dim=-1)    # (B, Classes)\n",
        "label = torch.tensor([2, 5], dtype=torch.int64)   # (B)\n",
        "loss = criterion(pred, label).to(device)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S1h88PE1zV_",
        "outputId": "658842cd-7c46-4bc0-899f-883a25440af1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3052, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training epoch"
      ],
      "metadata": {
        "id": "NzfwALEh3Tjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MANJBMIR3j5H"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  batch_iterator = tqdm(train_dataloader, desc=f\"Processing epoch {epoch:02d}\")\n",
        "  for batch in batch_iterator:\n",
        "    encoder_input = batch['encoder_input'].to(device)    # (B, seq_len)\n",
        "    decoder_input = batch['decoder_input'].to(device)    # (B, seq_len)\n",
        "    encoder_mask = batch['encoder_mask'].to(device)      # (B, 1, 1, seq_len)\n",
        "    decoder_mask = batch['decoder_mask'].to(device)      # (B, 1, seq_len, seq_len)\n",
        "\n",
        "    encoder_output = model.encode(encoder_input, encoder_mask)      # (B, seq_len, d_model)\n",
        "    decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     # (B, seq_len, d_model)\n",
        "    proj_output = model.project(decoder_output)     # (B, seq_len, tgt_vocab_size)\n",
        "\n",
        "    label = batch['label'].to(device)    # (B, seq_len)\n",
        "    loss = criterion(proj_output.view(-1, proj_output.shape[-1]), label.view(-1))\n",
        "    batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # update the weights\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lClimtL82E8h",
        "outputId": "c51491b6-006c-4813-b418-28eec638fbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 00:   0%|          | 60/28594 [00:49<6:29:19,  1.22it/s, loss=7.712]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECuHUbPB3xGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}