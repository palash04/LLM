{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7e37ca-b9df-4068-8881-d359a7afb20d",
   "metadata": {},
   "source": [
    "# Self Attention Step-by-step walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55feed01-9fb7-471a-87d2-e43de2e9da50",
   "metadata": {},
   "source": [
    "## Basic Self-attention without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6b0954-a625-4035-a2a8-6e995f809bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c118c8-1771-471a-b2b5-cf191294e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "T x Embed_Size\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([6, 3])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "   [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "print('-' * 100)\n",
    "print('T x Embed_Size')\n",
    "print('-' * 100)\n",
    "print(inputs.shape)\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dcccb-fd8b-44ab-95a6-26959d899938",
   "metadata": {},
   "source": [
    "### Compute attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54123586-690d-41be-b7c2-9c677cc8879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecb530d-3eeb-4dfa-b68c-39c18de0478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "T x T\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([6, 6])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-' * 100)\n",
    "print('T x T')\n",
    "print('-' * 100)\n",
    "print(attention_scores.shape)\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b8347-a93e-44ef-b53f-7905e77a5ec2",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "attention scores w21 -> w2T for input query x2, normalize to get attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18031684-feb1-4c30-8d58-b5e81f9b1436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x))\n",
    "attention_weights = torch.empty(attention_scores.shape)\n",
    "for i in range(attention_scores.shape[0]):\n",
    "    attention_weights[i] = softmax(attention_scores[i])\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342f9fad-55a1-4b02-a880-64df26e5a316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ecb931-10d2-45bb-b6e0-68dc819f96dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "469655a2-faac-4a93-897a-74c6dd12f63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6158a47c-ab15-4340-a1b7-388c9b28dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980720a-deeb-4180-b033-a0fec59c5d8a",
   "metadata": {},
   "source": [
    "### Compute the context vector\n",
    "\n",
    "z_2 = [a_21 * x_1] + [a22 * x_2] + ... [a2T * x_T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c00bbd2-aa41-4dd5-bbbc-39ca130dd6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_input_idx = 1\n",
    "query = inputs[query_input_idx]\n",
    "z_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    z_2 += x_i * attention_weights[query_input_idx][i]\n",
    "z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafc84c6-125b-4ea6-a91c-61bbe50c46ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = attention_weights @ inputs\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72340387-e675-4a29-8736-f33a81b89492",
   "metadata": {},
   "source": [
    "## Self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c333f-8e9b-4d99-bbb8-063f93f75f83",
   "metadata": {},
   "source": [
    "self-attention mechanism aka scaled-dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352120f-5c3b-4f28-b808-ccbe1061d940",
   "metadata": {},
   "source": [
    "#### 1. Compute query(q), key(k), value(v) vectors for input elements x\n",
    "\n",
    "For that we need to have trainable weight matrices Wq, Wk, Wv. These matrices project the embedded input tokens\n",
    "into query, key, value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d1bd15-71b0-479c-8385-7e46988afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one single query x_2\n",
    "x_2 = inputs[2]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbbec9c2-b36d-4fdf-8408-4a3f3e6659d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2540da4-6b49-447f-9447-975cbf5610c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize three weights matrices Wq, Wk, Wv\n",
    "torch.manual_seed(123)\n",
    "Wq = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "Wk = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "Wv = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8f5bac4-efa2-488c-9ea0-63223a70c14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf4f2de-fe47-4a59-b484-fb82c797952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute query, key, value vectors\n",
    "q = Wq @ x_2\n",
    "k = Wk @ x_2\n",
    "v = Wv @ x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab4dc46-26f0-4d75-affb-abcc10fafee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3]), torch.Size([6, 3]), torch.Size([6, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = inputs @ Wq\n",
    "keys = inputs @ Wk\n",
    "values = inputs @ Wv\n",
    "\n",
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34a358-2e17-40cb-8d89-fa14ec41ca62",
   "metadata": {},
   "source": [
    "#### 2. Compute attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a59fe9-1174-42a5-8f2f-6ca928657672",
   "metadata": {},
   "source": [
    "We compute dot product of query and key.\n",
    "\n",
    "Say, for for query_2 we want to find attention_scores \\\n",
    "attn_score_2_1 = dot(query_2, key_1) \\\n",
    "attn_score_2_2 = dot(query_2, key_2) \\\n",
    "... \\\n",
    "attn_score_2_T = dot(query_2, key_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ce32479-bbfa-4fef-8efd-67593e1e7465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = queries @ keys.T\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72556534-dbd1-42c6-ab30-a0e088987c29",
   "metadata": {},
   "source": [
    "#### 3. Compute attention weights\n",
    "\n",
    "Scale and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b84aa9b0-1cfb-49f4-b040-43b21c14d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1747, 0.1866, 0.1864, 0.1446, 0.1586, 0.1491],\n",
       "        [0.1862, 0.2123, 0.2117, 0.1179, 0.1450, 0.1269],\n",
       "        [0.1859, 0.2118, 0.2112, 0.1184, 0.1454, 0.1273],\n",
       "        [0.1798, 0.1936, 0.1932, 0.1365, 0.1542, 0.1427],\n",
       "        [0.1751, 0.1895, 0.1893, 0.1418, 0.1579, 0.1465],\n",
       "        [0.1837, 0.2003, 0.1998, 0.1293, 0.1501, 0.1369]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b699811a-ad70-44d2-9d97-555fafa5470e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "064e4454-d693-4a46-a365-9aaf445ee2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Compute the context vector (context maxtrix Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "312c5625-edb3-4726-9227-2b293d066179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4976, 0.9655, 0.7614],\n",
       "        [0.9074, 1.3518, 1.5075],\n",
       "        [0.8976, 1.3391, 1.4994],\n",
       "        [0.5187, 0.7319, 0.8493],\n",
       "        [0.4699, 0.7336, 0.9307],\n",
       "        [0.6446, 0.9045, 0.9814]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29eb96b9-2424-45ba-b355-26a5a54314a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0925, 0.1795, 0.1416])\n",
      "tensor([0.2847, 0.4658, 0.4608])\n",
      "tensor([0.4742, 0.7485, 0.7775])\n",
      "tensor([0.5356, 0.8352, 0.8780])\n",
      "tensor([0.6040, 0.9419, 1.0133])\n",
      "tensor([0.6860, 1.0570, 1.1383])\n"
     ]
    }
   ],
   "source": [
    "alpha_2 = attention_weights[2]\n",
    "z_2 = torch.zeros(3)\n",
    "\n",
    "for i, v_i in enumerate(values):\n",
    "    z_2 += alpha_2[i] * v_i\n",
    "    print(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13a7fd7a-fad3-47d3-b017-bd97dc44d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6692, 1.0276, 1.1106],\n",
       "        [0.6864, 1.0577, 1.1389],\n",
       "        [0.6860, 1.0570, 1.1383],\n",
       "        [0.6738, 1.0361, 1.1180],\n",
       "        [0.6711, 1.0307, 1.1139],\n",
       "        [0.6783, 1.0441, 1.1252]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = attention_weights @ values\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df29d6be-97f5-41dd-8edd-c56f331a72ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "T x d_model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([6, 3])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-' * 100)\n",
    "print('T x d_model')\n",
    "print('-' * 100)\n",
    "print(Z.shape)\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9180094-cd3a-4a9c-81df-73e38a1e5bd6",
   "metadata": {},
   "source": [
    "We add a normalization step to the dot product calculation in self-attention to aid training with large embedding sizes. This normalization, by the square root of the embedding dimension, is why it's called scaled-dot product attention.\n",
    "\n",
    "Here's what this means:\n",
    "\n",
    "* In natural language processing, we often use large embedding sizes (dimensions) to represent words.\n",
    "* When these dimensions are high (like in large language models), the dot product between query and key vectors can become very large.\n",
    "* Large dot products create problems during training because they make the softmax function act almost like an on/off switch. This, in turn, leads to very small gradients during backpropagation.\n",
    "* Small gradients make it difficult for the model to learn effectively.\n",
    "* By normalizing the dot product by the square root of the embedding dimension, we prevent these issues and ensure the gradients stay usable during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b4364-08b1-4205-8b01-bab0714b18cd",
   "metadata": {},
   "source": [
    "\"Why use query, key, and value?\n",
    "\n",
    "These terms, borrowed from information retrieval and databases, help organize and process information in attention mechanisms.\n",
    "\n",
    "A \"query\" is like a search term, representing the current focus of the model.\n",
    "A \"key\" indexes input items, aiding in matching with the query.\n",
    "The \"value\" holds the actual content or representation of input items, retrieved based on the matched keys.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4b198-b6d5-4110-9f64-587da4b57ed2",
   "metadata": {},
   "source": [
    "## Compact self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65c4936a-d9c7-43fa-a8b9-8b0d68714660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(143)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Wq = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.Wv = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.Wk = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (T, d_in) or (T, embed_size)\n",
    "        queries = x @ self.Wq  # (T, d_out)\n",
    "        keys = x @ self.Wk     # (T, d_out)\n",
    "        values = x @ self.Wv   # (T, d_out)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = queries @ keys.T  # (T, T)\n",
    "\n",
    "        # compute attention weights\n",
    "        attention_weights = torch.softmax( attention_scores / self.d_out ** 0.5 , 1) # (T, T)\n",
    "\n",
    "        # compute context vector\n",
    "        # Z_2 = (a2_1 * v_1) + (a2_2 * v_2) + .. (a2_T * v_T)\n",
    "        Z = attention_weights @ values\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41b3c493-10f3-4df6-8ecf-702e70cf7e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4608])\n"
     ]
    }
   ],
   "source": [
    "selfattn = SelfAttention(d_in=4608, d_out=4608)\n",
    "X = torch.rand(4096, 4608)\n",
    "Z = selfattn(X)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e258004-d12f-43ac-bdcd-d97f4ecabdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4608])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(143)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (T, d_in) or (T, embed_size)\n",
    "        queries = self.Wq(x)  # (T, d_out)\n",
    "        keys = self.Wk(x)     # (T, d_out)\n",
    "        values = self.Wv(x)   # (T, d_out)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = queries @ keys.T  # (T, T)\n",
    "\n",
    "        # compute attention weights\n",
    "        attention_weights = torch.softmax( attention_scores / self.d_out ** 0.5 , 1) # (T, T)\n",
    "\n",
    "        # compute context vector\n",
    "        # Z_2 = (a2_1 * v_1) + (a2_2 * v_2) + .. (a2_T * v_T)\n",
    "        Z = attention_weights @ values\n",
    "        return Z\n",
    "\n",
    "selfattn = SelfAttention(d_in=4608, d_out=4608, qkv_bias=False)\n",
    "X = torch.rand(4096, 4608)\n",
    "Z = selfattn(X)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ddbfa-f5e7-4af0-99d9-16131454e58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
