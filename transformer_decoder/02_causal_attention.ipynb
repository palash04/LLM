{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb5948a-8924-4637-813d-f07658bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0d2682-7183-4f20-a1aa-a1afc348b0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "T x Embed_Size\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([6, 3])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "   [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "print('-' * 100)\n",
    "print('T x Embed_Size')\n",
    "print('-' * 100)\n",
    "print(inputs.shape)\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b33dbaf-dd48-4a5c-8f1a-ee880e044e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = inputs.shape[1]\n",
    "\n",
    "Wq = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "Wk = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "Wv = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6c5117-84ea-4fd1-a65c-d8d18e8217c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3]), torch.Size([6, 3]), torch.Size([6, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = inputs @ Wq\n",
    "keys = inputs @ Wk\n",
    "values = inputs @ Wv\n",
    "\n",
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a83ca57-101b-44b2-a6ec-8de99f566f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7616, 0.8765, 0.8746, 0.4349, 0.5941, 0.4877],\n",
       "         [1.7872, 2.0141, 2.0091, 0.9952, 1.3538, 1.1227],\n",
       "         [1.7646, 1.9901, 1.9852, 0.9834, 1.3383, 1.1091],\n",
       "         [1.0664, 1.1947, 1.1916, 0.5897, 0.8004, 0.6667],\n",
       "         [0.8601, 0.9968, 0.9950, 0.4947, 0.6817, 0.5516],\n",
       "         [1.3458, 1.4957, 1.4915, 0.7374, 0.9968, 0.8366]]),\n",
       " torch.Size([6, 6]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = queries @ keys.T\n",
    "attention_scores, attention_scores.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ed782-56e8-4615-bade-cc00833d1f23",
   "metadata": {},
   "source": [
    "### Causal attention\n",
    "Causal attention, or masked attention, is a variant of self-attention where the model only looks at past and current tokens,\n",
    "unlike standard self-attention which considers the entire input sequence. This ensures that during attention computation,\n",
    "only tokens occurring before or at the current position are factored in.\n",
    "\n",
    "#### Steps for computing masked attention weights\n",
    "**Attention_Scores (unnormzalied)** -> (1) Mask with -âˆž above diagonal -> **Masked_Attention_Scores** -> (2) Apply Softmax -> **Masked_Attention_Weight** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57ee758b-e11a-482c-bf86-b53d63fd0bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = inputs.shape[0]   # Context length\n",
    "mask = torch.triu(torch.ones(T, T), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5368ae73-4179-45cb-b99c-3025a99c3118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7616,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [1.7872, 2.0141,   -inf,   -inf,   -inf,   -inf],\n",
       "        [1.7646, 1.9901, 1.9852,   -inf,   -inf,   -inf],\n",
       "        [1.0664, 1.1947, 1.1916, 0.5897,   -inf,   -inf],\n",
       "        [0.8601, 0.9968, 0.9950, 0.4947, 0.6817,   -inf],\n",
       "        [1.3458, 1.4957, 1.4915, 0.7374, 0.9968, 0.8366]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc351c-e715-494d-b201-493e470395a7",
   "metadata": {},
   "source": [
    "#### Normalize to get attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67a7e21-0186-4f2b-b8bc-437943b53faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4673, 0.5327, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3054, 0.3478, 0.3468, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2557, 0.2753, 0.2748, 0.1942, 0.0000, 0.0000],\n",
       "         [0.2051, 0.2220, 0.2217, 0.1661, 0.1851, 0.0000],\n",
       "         [0.1837, 0.2003, 0.1998, 0.1293, 0.1501, 0.1369]]),\n",
       " torch.Size([6, 6]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights = torch.softmax( masked / d_k ** 0.5 , dim=1 )\n",
    "attention_weights, attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b60a4c-04f7-43e3-a0ee-c81d8756742e",
   "metadata": {},
   "source": [
    "### Use modified attention weights to compute context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eda0330-332a-4426-bc41-8ae23602b856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4976, 0.9655, 0.7614],\n",
       "         [0.7159, 1.1712, 1.1589],\n",
       "         [0.7789, 1.2294, 1.2769],\n",
       "         [0.7244, 1.1291, 1.1867],\n",
       "         [0.6756, 1.0523, 1.1366],\n",
       "         [0.6783, 1.0441, 1.1252]]),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = attention_weights @ values\n",
    "Z, Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700c72b-55b9-485b-bd97-dbf8c48544e5",
   "metadata": {},
   "source": [
    "## Compact Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deb98cfd-3a66-4341-9ac0-dcaf6957ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Masked Attention Weights\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4902, 0.5098, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3391, 0.3367, 0.3242, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2667, 0.2456, 0.2456, 0.2421, 0.0000, 0.0000],\n",
      "        [0.2062, 0.2073, 0.2015, 0.1890, 0.1961, 0.0000],\n",
      "        [0.1847, 0.1625, 0.1674, 0.1682, 0.1513, 0.1659]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Z\n",
      "tensor([[-0.8628,  0.0772,  0.2997, -0.1688,  0.8274],\n",
      "        [-0.7920,  0.1168,  0.2625, -0.1527,  0.7410],\n",
      "        [-0.8819,  0.1180,  0.2414, -0.1056,  0.7409],\n",
      "        [-0.9319,  0.0779,  0.2426, -0.0683,  0.7029],\n",
      "        [-0.9264,  0.1476,  0.2210, -0.0710,  0.6686],\n",
      "        [-0.9599,  0.1291,  0.2190, -0.0369,  0.6332]], grad_fn=<MmBackward0>)\n",
      "torch.Size([6, 5])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(143)\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super(CausalAttention, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Wq = nn.Linear(d_in, d_out)\n",
    "        self.Wv = nn.Linear(d_in, d_out)\n",
    "        self.Wk = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (T, d_in) or (T, embed_size)\n",
    "        T = x.shape[0]\n",
    "        queries = self.Wq(x)  # (T, d_out)\n",
    "        keys = self.Wk(x)     # (T, d_out)\n",
    "        values = self.Wv(x)   # (T, d_out)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = queries @ keys.T  # (T, T)\n",
    "\n",
    "        # compute masked attention weights\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1) # (T, T)\n",
    "        masked_attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        \n",
    "        # compute attention weights\n",
    "        attention_weights = torch.softmax( masked_attention_scores / self.d_out ** 0.5 , 1) # (T, T)\n",
    "        print('-'* 100)\n",
    "        print('Masked Attention Weights')\n",
    "        print(attention_weights)\n",
    "        print('-'* 100)\n",
    "\n",
    "        # compute context vector\n",
    "        # Z_2 = (a2_1 * v_1) + (a2_2 * v_2) + .. (a2_T * v_T)\n",
    "        Z = attention_weights @ values\n",
    "        return Z\n",
    "\n",
    "selfattn = CausalAttention(d_in=5, d_out=5, qkv_bias=False)\n",
    "X = torch.rand(6, 5)\n",
    "Z = selfattn(X)\n",
    "print('-'* 100)\n",
    "print('Z')\n",
    "print(Z)\n",
    "print(Z.shape)\n",
    "print('-'* 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a134a-10b5-4a03-8e1e-c0a4d9507af2",
   "metadata": {},
   "source": [
    "### Dropout in attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d54e7-547d-4fa3-a461-3aa8b9a65981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(143)\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False, dropout=0.5):\n",
    "        super(CausalAttention, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Wq = nn.Linear(d_in, d_out)\n",
    "        self.Wv = nn.Linear(d_in, d_out)\n",
    "        self.Wk = nn.Linear(d_in, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> (T, d_in) or (T, embed_size)\n",
    "        T = x.shape[0]\n",
    "        queries = self.Wq(x)  # (T, d_out)\n",
    "        keys = self.Wk(x)     # (T, d_out)\n",
    "        values = self.Wv(x)   # (T, d_out)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = queries @ keys.T  # (T, T)\n",
    "\n",
    "        # compute masked attention weights\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1) # (T, T)\n",
    "        masked_attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        \n",
    "        # compute attention weights\n",
    "        attention_weights = torch.softmax( masked_attention_scores / self.d_out ** 0.5 , 1) # (T, T)\n",
    "        print('-'* 100)\n",
    "        print('Masked Attention Weights')\n",
    "        print(attention_weights)\n",
    "        print('-'* 100)\n",
    "\n",
    "        # compute context vector\n",
    "        # Z_2 = (a2_1 * v_1) + (a2_2 * v_2) + .. (a2_T * v_T)\n",
    "        Z = attention_weights @ values\n",
    "        return Z\n",
    "\n",
    "selfattn = CausalAttention(d_in=5, d_out=5, qkv_bias=False)\n",
    "X = torch.rand(6, 5)\n",
    "Z = selfattn(X)\n",
    "print('-'* 100)\n",
    "print('Z')\n",
    "print(Z)\n",
    "print(Z.shape)\n",
    "print('-'* 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
