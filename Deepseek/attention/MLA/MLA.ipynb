{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "WHhDfMmrelwD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  d_model: int = 5120\n",
        "  n_heads: int = 32\n",
        "  kv_compression_dim: int = 512\n",
        "  q_compression_dim: int = 512\n",
        "  rope_dim: int = 64"
      ],
      "metadata": {
        "id": "Vp25Lmbye3MQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.d_model = config.d_model\n",
        "    self.n_heads = config.n_heads\n",
        "\n",
        "    assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.head_dim = self.d_model // self.n_heads     # d_h (dimension_per_head)\n",
        "\n",
        "    self.kv_compression_dim = config.kv_compression_dim    # d_c (KV compression dimension)\n",
        "    self.q_compression_dim = config.q_compression_dim      # d'_c (query compression dimension)\n",
        "    self.rope_dim = config.rope_dim        # d_R (decoupled RoPE vector dimension per head)\n",
        "\n",
        "    # 1. Down projection for keys and values\n",
        "    self.w_DKV = nn.Linear(self.d_model, self.kv_compression_dim)   # d_model -> d_c\n",
        "\n",
        "    # 2. Up projection for keys\n",
        "    self.w_UK = nn.Linear(self.kv_compression_dim, self.d_model)    # d_c -> d_model\n",
        "\n",
        "    # 3. Up projection for values\n",
        "    self.w_UV = nn.Linear(self.kv_compression_dim, self.d_model)    # d_c -> d_model\n",
        "\n",
        "    # 4. Decoupled key projection\n",
        "    self.w_KR = nn.Linear(self.d_model, self.n_heads * self.rope_dim)    # d_model -> nh * d_R\n",
        "\n",
        "    # 5. Down projection for queries\n",
        "    self.w_DQ = nn.Linear(self.d_model, self.q_compression_dim)   # d_model -> d'_c\n",
        "\n",
        "    # 6. Up projection for queries\n",
        "    self.w_UQ = nn.Linear(self.q_compression_dim, self.d_model)   # d'_c -> d_model\n",
        "\n",
        "    # 7. Decoupled query projection\n",
        "    self.w_QR = nn.Linear(self.q_compression_dim, self.n_heads * self.rope_dim)   # d'_c -> nh * d_R\n",
        "\n",
        "    # 8. Output projection\n",
        "    self.w_o = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, T, d_model)\n",
        "    B, T, _ = x.shape\n",
        "\n",
        "    # compute compressed latent for keys and values\n",
        "    c_KV = self.w_DKV(x)    # (B, T, d_c)\n",
        "\n",
        "    # up-project c_KV to get keys and values for all heads\n",
        "    k_C = self.w_UK(c_KV)   # (B, T, d_model)\n",
        "    value = self.w_UV(c_KV)   # (B, T, d_model)\n",
        "\n",
        "    # reshape k_C, value to (B, nh, T, head_dim)\n",
        "    k_C = k_C.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, nh, T, head_dim)\n",
        "    value = value.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, nh, T, head_dim)\n",
        "\n",
        "    # compute decoupled positional key component from original input\n",
        "    k_R = self.w_KR(x)   # (B, T, nh * d_R)\n",
        "    k_R = k_R.reshape(B, T, self.n_heads, self.rope_dim)   # (B, T, nh, d_R)\n",
        "    k_R = k_R.transpose(1, 2)    # (B, nh, T, d_R)\n",
        "\n",
        "    # Apply RoPE to k_R: k_R = ROPE(k_R)   ...Skipping for now.\n",
        "\n",
        "    # compute compressed latent for queries\n",
        "    c_q = self.w_DQ(x)    # (B, T, d'_c)\n",
        "\n",
        "    # up-project q_C to get query components for all heads\n",
        "    q_C_all = self.w_UQ(c_q)    # (B, T, d_model)\n",
        "\n",
        "    # reshape\n",
        "    q_C = q_C_all.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)   # (B, nh, T, hd)\n",
        "\n",
        "    q_R_all = self.w_QR(c_q)     # (B, T, nh * d_R)\n",
        "    q_R = q_R_all.view(B, T, self.n_heads, self.rope_dim)   # (B, T, nh, d_R)\n",
        "    q_R = q_R.transpose(1, 2)    # (B, nh, T, d_R)\n",
        "\n",
        "    # Apply RoPE to q_R: q_R = ROPE(q_R)   ...Skipping for now.\n",
        "\n",
        "    query = torch.cat([q_C, q_R], dim=-1)   # (B, nh, T, hd + d_R)\n",
        "    key = torch.cat([k_C, k_R], dim=-1)   # (B, nh, T, hd + d_R)\n",
        "\n",
        "    attention_scores = query @ key.transpose(2, 3) / math.sqrt(key.shape[-1])   # (B, nh, T, T)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)   # (B, nh, T, T)\n",
        "\n",
        "    z = attention_scores @ value    # (B, nh, T, d_model)\n",
        "\n",
        "    z = z.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.head_dim)   # (B, T, d_model)\n",
        "\n",
        "    return self.w_o(z)    # (B, T, d_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "n5VBJjuxeloN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "mla_attn = MultiHeadLatentAttention(config)\n",
        "mla_attn"
      ],
      "metadata": {
        "id": "6EpZuKAKelfj",
        "outputId": "29ae35d5-1d3b-4bd0-8cb6-b44bf1b6a725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadLatentAttention(\n",
              "  (w_DKV): Linear(in_features=5120, out_features=512, bias=True)\n",
              "  (w_UK): Linear(in_features=512, out_features=5120, bias=True)\n",
              "  (w_UV): Linear(in_features=512, out_features=5120, bias=True)\n",
              "  (w_KR): Linear(in_features=5120, out_features=2048, bias=True)\n",
              "  (w_DQ): Linear(in_features=5120, out_features=512, bias=True)\n",
              "  (w_UQ): Linear(in_features=512, out_features=5120, bias=True)\n",
              "  (w_QR): Linear(in_features=512, out_features=2048, bias=True)\n",
              "  (w_o): Linear(in_features=5120, out_features=5120, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "SEQ_LEN = 4096\n",
        "D_MODEL = config.d_model\n",
        "X = torch.randn(BATCH_SIZE, SEQ_LEN, D_MODEL)"
      ],
      "metadata": {
        "id": "pY-DCotCV3D4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = mla_attn(X)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "-F0Yg1_eWIZ1",
        "outputId": "e5d522fa-6293-4b48-88f1-c68c7f9b6bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4096, 5120])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HqsCeJPWKl1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}