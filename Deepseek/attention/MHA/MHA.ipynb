{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "bBJmMUU4WjOf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  vocab_size: int = 50000\n",
        "  seq_len: int = 4096\n",
        "  d_model: int = 5120\n",
        "  n_heads: int = 32\n",
        "  n_kv_heads: int = 8\n",
        "  n_layers: int = 40\n",
        "  hidden_size: int = 14336"
      ],
      "metadata": {
        "id": "XO7qJ8UDWjLN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = config.d_model\n",
        "    self.n_heads = config.n_heads\n",
        "    assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "    self.w_q = nn.Linear(self.d_model, self.n_heads * self.head_dim, bias=False)\n",
        "    self.w_k = nn.Linear(self.d_model, self.n_heads * self.head_dim, bias=False)\n",
        "    self.w_v = nn.Linear(self.d_model, self.n_heads * self.head_dim, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(self.n_heads * self.head_dim, self.d_model, bias=False)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # x: (B, T, d_model)\n",
        "    # mask: (B, 1, T, T)\n",
        "\n",
        "    B, T, _ = x.shape\n",
        "    query = self.w_q(x)    # (B, T, nh * hd)\n",
        "    key = self.w_k(x)    # (B, T, nh * hd)\n",
        "    value = self.w_v(x)    # (B, T, nh * hd)\n",
        "\n",
        "    query = query.view(B, T, self.n_heads, self.head_dim)    # (B, T, nh, hd)\n",
        "    key = key.view(B, T, self.n_heads, self.head_dim)    # (B, T, nh, hd)\n",
        "    value = value.view(B, T, self.n_heads, self.head_dim)    # (B, T, nh, hd)\n",
        "\n",
        "    query = query.transpose(1, 2)    # (B, nh, T, hd)\n",
        "    key = key.transpose(1, 2)        # (B, nh, T, hd)\n",
        "    value = value.transpose(1, 2)    # (B, nh, T, hd)\n",
        "\n",
        "    attention_scores = query @ key.transpose(2, 3) / math.sqrt(self.head_dim)   # (B, nh, T, T)\n",
        "\n",
        "    # apply mask\n",
        "    if mask is not None:\n",
        "      attention_scores = attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    z = attention_scores @ value    # (B, nh, T, hd)\n",
        "    z = z.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.head_dim)    # (B, T, nh * hd)\n",
        "\n",
        "    return self.w_o(z)    # (B, T, d_model)"
      ],
      "metadata": {
        "id": "3nbRfxAyWjHN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones(1, size, size), diagonal=1)\n",
        "  return mask == 0"
      ],
      "metadata": {
        "id": "cRLzgR3JWjAc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "mask = causal_mask(config.seq_len)\n",
        "mask = mask.unsqueeze(0)\n",
        "mask.shape  # (B, nh, T, T)"
      ],
      "metadata": {
        "id": "NkRIff3Ge5sk",
        "outputId": "acca4291-284e-4dfb-cec5-cdc37e1af326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4096, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "SEQ_LEN = config.seq_len\n",
        "D_MODEL = config.d_model\n",
        "X = torch.randn(BATCH_SIZE, SEQ_LEN, D_MODEL)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "BaxcHpDiJw6x",
        "outputId": "7a131c9f-588e-499a-ce6b-0965fd75715a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4096, 5120])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn = MultiHeadAttentionBlock(config)\n",
        "output = attn(X, mask)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "oE4SrhkiLC28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrJhk1OhLJLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}