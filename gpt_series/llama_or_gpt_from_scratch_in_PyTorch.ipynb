{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook contains smaller size llama/gpt model without rope.\n",
        "Contains:\n",
        "- GQA\n",
        "- FFN with SWIGLU"
      ],
      "metadata": {
        "id": "LjKfSA98-y_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "AwmWvlvEo2SA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  vocab_size: int = 50000\n",
        "  d_model: int = 512\n",
        "  seq_len: int = 1024\n",
        "  n_heads: int = 8\n",
        "  kv_n_heads: int = None\n",
        "  n_layers: int = 6\n",
        "  dropout: float = 0.1\n",
        "  expansion_ratio: int = 4\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "2rnq-f8UUkIA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InputEmbeddings\n"
      ],
      "metadata": {
        "id": "I3IeXo7GUEnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    vocab_size = config.vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, T) -> (B, T, d_model)\n",
        "    return self.embedding(x)\n",
        "\n",
        "x = torch.randint(low=0, high=config.vocab_size, size=(2, config.seq_len))\n",
        "x_embed = InputEmbeddings(config=config)\n",
        "out = x_embed(x)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "LAOPVQGiJCPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5cef3b6-b461-4f02-81f7-957cab022700"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PositionalEncoding"
      ],
      "metadata": {
        "id": "Brk6-QZpUKCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    seq_len = config.seq_len\n",
        "    d_model = config.d_model\n",
        "\n",
        "\n",
        "    # create a positional encoding tensor of shape (T, d_model)\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    # create a vector of shape (seq_len)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)    # (seq_len, 1)\n",
        "\n",
        "    # create a vector of shape (d_model)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).unsqueeze(0)  # (1, d_model / 2)\n",
        "\n",
        "    # apply sine to even indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "    # apply cosine to odd indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)    # (1, T, d_model)\n",
        "\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, d_model)\n",
        "    x  = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)    # (B, T, d_model)\n",
        "    return x\n",
        "\n",
        "\n",
        "x = torch.rand(2, config.seq_len, config.d_model)\n",
        "pos = PositionalEncoding(config=config)\n",
        "out = pos(x)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "dhgcvTVXo2Pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d411b66c-9306-4512-8124-790ad16f46ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoPE"
      ],
      "metadata": {
        "id": "wNBO4KEwVflf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Coming soon....\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "C6sWe9KdVhFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30ed6191-b41b-4632-c0a0-9affe4d0fa89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nComing soon....\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSNormalization"
      ],
      "metadata": {
        "id": "-Rq1bT9RUel3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNormalization(nn.Module):\n",
        "  def __init__(self, config: Config, eps: float=1e-6):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "  def _norm(self, x):\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, d_model)\n",
        "    return self.gamma * self._norm(x.float()).type_as(x)\n",
        "\n",
        "x = torch.rand(2, config.seq_len, config.d_model)\n",
        "norm = RMSNormalization(config=config)\n",
        "out = norm(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFrgCynbYWmf",
        "outputId": "80398278-2ef5-4408-f08f-e4b839ba26ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForwardBlock with ReLU"
      ],
      "metadata": {
        "id": "XFhOYLf3UPKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    expansion_ratio = config.expansion_ratio\n",
        "    dropout = config.dropout\n",
        "    hidden_size = expansion_ratio * d_model\n",
        "\n",
        "    self.up_proj = nn.Linear(d_model, hidden_size)\n",
        "    self.down_proj = nn.Linear(hidden_size, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, d_ff) -> (B, T, d_model)\n",
        "    x = self.dropout(self.relu(self.up_proj(x)))\n",
        "    x = self.down_proj(x)\n",
        "    return x\n",
        "\n",
        "x = torch.rand(1, config.seq_len, config.d_model)\n",
        "ffn = FeedForwardBlock(config=config)\n",
        "out = ffn(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkjcmMmUPnP",
        "outputId": "b9c32cfa-3ebc-43cc-9fd8-91ab74163a64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForwardBlock with SWIGLU"
      ],
      "metadata": {
        "id": "6NPuPkzhcmh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardSwigluBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    d_model = config.d_model\n",
        "    expansion_ratio = config.expansion_ratio\n",
        "    hidden_size = expansion_ratio * d_model\n",
        "\n",
        "    self.w1 = nn.Linear(d_model, hidden_size, bias=False)\n",
        "    self.w2 = nn.Linear(hidden_size, d_model, bias=False)\n",
        "    self.w3 = nn.Linear(d_model, hidden_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, hidden_size) -> (B, T, d_model)\n",
        "    swish = F.silu(self.w1(x))   # (B, T, hidden_size)\n",
        "    x_V = self.w3(x)   # (B, T, hidden_size)\n",
        "    x = swish * x_V    # (B, T, hidden_size)\n",
        "    return self.w2(x)   # (B, T, d_model)\n",
        "\n",
        "x = torch.rand(1, config.seq_len, config.d_model)\n",
        "ffn = FeedForwardSwigluBlock(config=config)\n",
        "out = ffn(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJyBgQLVcmXr",
        "outputId": "e2824c92-3f7f-4410-b8b0-d714501509db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzRPZjU4cmOE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttentionBlock"
      ],
      "metadata": {
        "id": "TLxoSLk-UMOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.d_model = config.d_model\n",
        "    self.n_heads = config.n_heads\n",
        "    dropout = config.dropout\n",
        "    assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.head_size = self.d_model // self.n_heads\n",
        "\n",
        "    self.kv_n_heads = config.n_heads if config.kv_n_heads is None else config.kv_n_heads\n",
        "\n",
        "    # Indicates how many times keys and values should be repeated\n",
        "    self.n_rep = self.n_heads // self.kv_n_heads\n",
        "\n",
        "    self.w_q = nn.Linear(self.d_model, self.n_heads * self.head_size, bias=False)\n",
        "    self.w_k = nn.Linear(self.d_model, self.kv_n_heads * self.head_size, bias=False)\n",
        "    self.w_v = nn.Linear(self.d_model, self.kv_n_heads * self.head_size, bias=False)\n",
        "    self.w_o = nn.Linear(self.n_heads * self.head_size, self.d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def repeat_kv(self, x: torch.Tensor, n_rep: int):\n",
        "    B, T, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "      return x\n",
        "\n",
        "    x = x[:, :, :, None, :].expand(B, T, n_kv_heads, n_rep, head_dim).reshape(B, T, n_kv_heads * n_rep, head_dim)   # (B, T, n_heads, head_dim)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # x: (B, T, d_model)\n",
        "    # mask: (B, 1, seq_len, seq_len)\n",
        "    B, T, _ = x.shape\n",
        "    query = self.w_q(x)    # (B, T, nH * hd)\n",
        "    key = self.w_k(x)      # (B, T, nKVH * hd)\n",
        "    value = self.w_v(x)    # (B, T, nKVH * hd)\n",
        "\n",
        "    query = query.view(B, T, self.n_heads, self.head_size)    # (B, T, nH, Hd)\n",
        "    key = key.view(B, T, self.kv_n_heads, self.head_size)        # (B, T, nKVH, Hd)\n",
        "    value = value.view(B, T, self.kv_n_heads, self.head_size)    # (B, T, nKVH, Hd)\n",
        "\n",
        "    # Since every group of Q shares same K and V heads, just repeat the K and V heads for every Q in the same group\n",
        "    key = self.repeat_kv(key, self.n_rep)             # (B, T, nH, Hd)\n",
        "    value = self.repeat_kv(value, self.n_rep)         # (B, T, nH, Hd)\n",
        "\n",
        "    query = query.transpose(1, 2)   # (B, nH, T, Hd)\n",
        "    key = key.transpose(1, 2)       # (B, nH, T, Hd)\n",
        "    value = value.transpose(1, 2)   # (B, nH, T, Hd)\n",
        "\n",
        "    # compute the attention score\n",
        "    attention_scores = query @ key.transpose(2, 3) / math.sqrt(self.head_size)    # (B, nH, T, T)\n",
        "\n",
        "    # apply mask\n",
        "    if mask is not None:\n",
        "      attention_scores = attention_scores.masked_fill_(mask == 0, -1e9)     # (B, nH, T, T)\n",
        "\n",
        "    if self.dropout is not None:\n",
        "      attention_scores = self.dropout(attention_scores)    # (B, nH, T, T)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)    # (B, nH, T, T)\n",
        "\n",
        "    # compute the context vector\n",
        "    z = attention_scores @ value      # (B, nH, T, Hd)\n",
        "\n",
        "    z = z.transpose(1, 2).contiguous().view(B, T, self.head_size * self.n_heads)    # (B, T, d_model)\n",
        "\n",
        "    return self.w_o(z)\n",
        "\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones(1, size, size), diagonal=1)\n",
        "  return mask == 0\n",
        "\n",
        "## Without GQA (as in MHA)\n",
        "config.kv_n_heads = config.n_heads\n",
        "x = torch.rand(1, config.seq_len, config.d_model)\n",
        "mask = causal_mask(config.seq_len)\n",
        "attn = MultiHeadAttentionBlock(config=config)\n",
        "out = attn(x, mask)\n",
        "print(out.shape)\n",
        "\n",
        "## With GQA\n",
        "config.kv_n_heads = 4\n",
        "x = torch.rand(1, config.seq_len, config.d_model)\n",
        "mask = causal_mask(config.seq_len)\n",
        "attn = MultiHeadAttentionBlock(config=config)\n",
        "out = attn(x, mask)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7AzVGFCo73U",
        "outputId": "948f5088-26af-49c9-cc1a-aeb4518f2e22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024, 512])\n",
            "torch.Size([1, 1024, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K7Xvo1Lxo2Mr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DecoderBlock"
      ],
      "metadata": {
        "id": "2DXeHIy0UUYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.norm1 = RMSNormalization(config)\n",
        "    self.norm2 = RMSNormalization(config)\n",
        "    self.attn = MultiHeadAttentionBlock(config)\n",
        "    self.ffn = FeedForwardSwigluBlock(config)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # x: (B, T, d_model)\n",
        "    # mask: (B, 1, seq_len, seq_len)\n",
        "    x = x + self.attn(self.norm1(x), mask)\n",
        "    x = x + self.ffn(self.norm2(x))\n",
        "    return x\n",
        "\n",
        "x = torch.rand(1, config.seq_len, config.d_model)\n",
        "mask = causal_mask(config.seq_len)\n",
        "decoder_block = DecoderBlock(config=config)\n",
        "out = decoder_block(x, mask)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "IFvIkswzo2IV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d2b795-690d-4d67-9647-60367f39b010"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "cohL-BbZUW23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config: Config, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = RMSNormalization(config)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "Bd1YpM7mo1-1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ProjectionLayer"
      ],
      "metadata": {
        "id": "CsZlK98773CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (B, T, d_model) -> (B, T, vocab_size)\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "id": "twlhmdku72bV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaModel"
      ],
      "metadata": {
        "id": "KlKRtt60UYOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6IYLHEhWjqqC"
      },
      "outputs": [],
      "source": [
        "class LLAMA(nn.Module):\n",
        "  def __init__(self, embed: InputEmbeddings, pos_enc: PositionalEncoding, decoder: Decoder, projection_layer: ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.embed = embed\n",
        "    self.pos_enc = pos_enc\n",
        "    self.decoder = decoder\n",
        "    self.projection = projection_layer\n",
        "\n",
        "  def decode(self, x, mask):\n",
        "    # x: (B, T)\n",
        "    # mask: (B, 1, seq_len, seq_len)\n",
        "    x_emb = self.embed(x)\n",
        "    x_pos = self.pos_enc(x_emb)\n",
        "    x = self.decoder(x_pos, mask)\n",
        "    return x\n",
        "\n",
        "  def project(self, x):\n",
        "    # (B, T, d_model) -> (B, T, vocab_size)\n",
        "    return self.projection(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bulid Llama model"
      ],
      "metadata": {
        "id": "gSvpKxkJUaib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_llama(config: Config):\n",
        "\n",
        "  embed = InputEmbeddings(config)\n",
        "  pos_enc = PositionalEncoding(config)\n",
        "\n",
        "  decoder_blocks = []\n",
        "  for _ in range(config.n_layers):\n",
        "    decoder_block = DecoderBlock(config)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "  decoder = Decoder(config, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  projection_layer = ProjectionLayer(config)\n",
        "\n",
        "  model = LLAMA(embed, pos_enc, decoder, projection_layer)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "zo28mijyUa1d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_llama(config)\n",
        "model"
      ],
      "metadata": {
        "id": "C76Unv6qUbIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529449ad-fdd8-4996-d403-411f99884984"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLAMA(\n",
              "  (embed): InputEmbeddings(\n",
              "    (embedding): Embedding(50000, 512)\n",
              "  )\n",
              "  (pos_enc): PositionalEncoding()\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (norm1): RMSNormalization()\n",
              "        (norm2): RMSNormalization()\n",
              "        (attn): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=256, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=256, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): FeedForwardSwigluBlock(\n",
              "          (w1): Linear(in_features=512, out_features=2048, bias=False)\n",
              "          (w2): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (w3): Linear(in_features=512, out_features=2048, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): RMSNormalization()\n",
              "  )\n",
              "  (projection): ProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=50000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = 1\n",
        "x = torch.randint(low=0, high=config.vocab_size, size=(B, config.seq_len))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T5RhROzjBGp",
        "outputId": "083bdf25-77a2-4b90-faa4-4c1d0d84c2b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = causal_mask(config.seq_len)\n",
        "mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLpjBBFCZ3J-",
        "outputId": "730842da-a0d6-4c6d-c7a1-68c62ef65039"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = mask.unsqueeze(0)     # (batch dimension)\n",
        "mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnO1id__aFEg",
        "outputId": "16b2b044-8000-411a-c3f8-a49407013db6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1024, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.decode(x, mask)\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qaa4kcGhaOtb",
        "outputId": "524626c1-c027-4dce-c23f-82821f242569"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.project(logits)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW69INsfaUrN",
        "outputId": "630076f8-2041-4448-8eac-c89295d8edb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 50000])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute loss, acc, ....\n",
        "# Done!!"
      ],
      "metadata": {
        "id": "ndf4DsF3afBF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdPClwoHakxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}